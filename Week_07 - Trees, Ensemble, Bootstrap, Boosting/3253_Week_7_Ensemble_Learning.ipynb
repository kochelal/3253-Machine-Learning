{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOCoBwVUf5z9Std5i5iV4jT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saeid-uot/3253-Machine-Learning/blob/main/Week_07%20-%20Trees%2C%20Ensemble%2C%20Bootstrap%2C%20Boosting/3253_Week_7_Ensemble_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Objective:\n",
        "Below Python code aims to demonstrate the implementation and evaluation of various ensemble learning techniques, including Decision Tree, Random Forest, and Extra Trees classifiers, on a synthetic dataset.\n",
        "\n",
        "Step 1: Initially, the dataset is generated and balanced using the Synthetic Minority Over-sampling Technique (SMOTE).\n",
        "\n",
        "Steps that are omitted: There are several steps here, including description, visualization to better understand data, deal with outlier, missing values, scaling, etc etc. We skip them for the sake of simplicity for now\n",
        "\n",
        "Step 2: The dataset is split into training and testing sets.\n",
        "\n",
        "\n",
        "Step 3: Individual models are then trained and evaluated using metrics such as accuracy, precision, recall, F1 score, and area under the ROC curve (AUC).\n",
        "\n",
        "Step 4: Finally, the results from each model are aggregated into a tabular format for straightforward comparison, facilitating an understanding of the performance differences between the ensemble learning methods employed."
      ],
      "metadata": {
        "id": "sPyT6HkaavxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Loading Packages and Generate a dataset"
      ],
      "metadata": {
        "id": "hOfxydipbV1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load dataset and balance the data using SMOTE\n",
        "from sklearn.datasets import make_classification\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Generating a synthetic dataset\n",
        "# class_sep   default=1.0 The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task easier.\n",
        "# Learning mode about make_classification here https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html\n",
        "\n",
        "X, y = make_classification(n_samples=5000, n_features=25,class_sep=0.01, n_classes=2, weights=[0.9, 0.1], random_state=42)\n",
        "X, y = make_classification(n_samples=8000, n_features=30,class_sep=0.001, n_classes=2, weights=[0.9, 0.1], random_state=42)\n",
        "# Balancing the dataset using SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)"
      ],
      "metadata": {
        "id": "tZAKQZCebTpu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Split the dataset"
      ],
      "metadata": {
        "id": "FCvzJu_NbVHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 2: Split the dataset into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "#Clean memory up to ensure it won't slow down your work!\n",
        "del X_resampled,y_resampled"
      ],
      "metadata": {
        "id": "tdsbel1fbmlU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3A - Train and Evaluate a **Decision Tree**"
      ],
      "metadata": {
        "id": "pI4x0WhUbnKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Build and evaluate the Decision Tree model\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Decision Tree\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "dt_clf.fit(X_train, y_train)\n",
        "dt_preds = dt_clf.predict(X_test)\n",
        "\n",
        "dt_cm = confusion_matrix(y_test, dt_preds)\n",
        "\n",
        "# Evaluation metrics for Decision Tree\n",
        "dt_metrics = {\n",
        "    \"Model\": \"Decision Tree\",\n",
        "    \"Accuracy\": accuracy_score(y_test, dt_preds),\n",
        "    \"Precision\": precision_score(y_test, dt_preds),\n",
        "    \"Recall\": recall_score(y_test, dt_preds),\n",
        "    \"F1 Score\": f1_score(y_test, dt_preds),\n",
        "    \"AUC\": roc_auc_score(y_test, dt_preds)\n",
        "}\n",
        "\n",
        "print(dt_metrics,\"\\n\",\"Confusion Matrix:\\n\", dt_cm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21PWdBg7bncH",
        "outputId": "2848e924-e2ba-4f11-c409-7324ef99e115"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Model': 'Decision Tree', 'Accuracy': 0.7919720767888307, 'Precision': 0.7706598334401025, 'Recall': 0.8348369188063844, 'F1 Score': 0.8014656895403065, 'AUC': 0.791716212212181} \n",
            " Confusion Matrix:\n",
            " [[1066  358]\n",
            " [ 238 1203]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3A - Train and Evaluate a **Vanilla SVM Model**"
      ],
      "metadata": {
        "id": "8rNZbCkAcKJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Build and evaluate the Decision Tree model\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Decision Tree\n",
        "svc_clf = SVC(probability=True, random_state=42)\n",
        "svc_clf.fit(X_train, y_train)\n",
        "svc_preds = svc_clf.predict(X_test)\n",
        "\n",
        "svc_cm = confusion_matrix(y_test, svc_preds)\n",
        "\n",
        "# Evaluation metrics for Decision Tree\n",
        "svc_metrics = {\n",
        "    \"Model\": \"Support Vector Machine\",\n",
        "    \"Accuracy\": accuracy_score(y_test, svc_preds),\n",
        "    \"Precision\": precision_score(y_test, svc_preds),\n",
        "    \"Recall\": recall_score(y_test, svc_preds),\n",
        "    \"F1 Score\": f1_score(y_test, svc_preds),\n",
        "    \"AUC\": roc_auc_score(y_test, svc_preds)\n",
        "}\n",
        "\n",
        "print(svc_metrics,\"\\n\",\"Confusion Matrix:\\n\", svc_cm)\n",
        "print(\">>>>>>>>>>>>>There is a change of F1 score from \", dt_metrics['F1 Score'],\" to \",svc_metrics['F1 Score'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbqAYHcdcME-",
        "outputId": "5cb532b3-a222-48d9-f6c0-939082a9c925"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Model': 'Support Vector Machine', 'Accuracy': 0.9102966841186736, 'Precision': 0.8794871794871795, 'Recall': 0.952116585704372, 'F1 Score': 0.9143618793735422, 'AUC': 0.9100470568971296} \n",
            " Confusion Matrix:\n",
            " [[1236  188]\n",
            " [  69 1372]]\n",
            ">>>>>>>>>>>>>There is a change of F1 score from  0.8014656895403065  to  0.9143618793735422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3C - Tune and Evaluate a **SVC With Kernel**"
      ],
      "metadata": {
        "id": "XSX_ecZwdTbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and evaluate the Support Vector Machine model with hyperparameter tuning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],  # Regularization parameter\n",
        "    'gamma': [0.1, 0.01],  # Kernel coefficient\n",
        "    'kernel': ['rbf']  # Kernel type\n",
        "}\n",
        "\n",
        "# Initialize the SVM classifier\n",
        "svc_clf = SVC(probability=True,random_state=42)\n",
        "\n",
        "# Perform grid search with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(svc_clf, param_grid, cv=3, scoring='f1', verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best estimator\n",
        "best_svc_clf = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "tuned_svc_preds = best_svc_clf.predict(X_test)\n",
        "\n",
        "tuned_svc_cm = confusion_matrix(y_test, tuned_svc_preds)\n",
        "\n",
        "# Evaluation metrics for Support Vector Machine\n",
        "tuned_svc_metrics = {\n",
        "    \"Model\": \"Support Vector Machine\",\n",
        "    \"Accuracy\": accuracy_score(y_test, tuned_svc_preds),\n",
        "    \"Precision\": precision_score(y_test, tuned_svc_preds),\n",
        "    \"Recall\": recall_score(y_test, tuned_svc_preds),\n",
        "    \"F1 Score\": f1_score(y_test, tuned_svc_preds),\n",
        "    \"AUC\": roc_auc_score(y_test, tuned_svc_preds)\n",
        "}\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print()\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Evaluation Metrics for Support Vector Machine:\")\n",
        "print(tuned_svc_metrics,\"\\n\",\"Confusion Matrix:\\n\", tuned_svc_cm)\n",
        "\n",
        "#Slightly different way of printing; we are using 'f' in the beginning to directly embed python objects inside a {} within the quote itself instead of separating them using comma\n",
        "print(f\">>>>>>>>>>>>> For {dt_metrics['Model']}, there is a change of F1 score from {dt_metrics['F1 Score']} to {tuned_svc_metrics['F1 Score']} compared to {tuned_svc_metrics['Model']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcFKu1ModWGz",
        "outputId": "5236ddd3-870e-4c75-f1ce-ba9fc1ddb4fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compare AUC of Decision Tree, Vanilla SVC without hyperparameter tuning, and SVC with kernel which is tuned"
      ],
      "metadata": {
        "id": "tX0oNoqYix0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Initialize a plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot ROC curve and calculate AUC for Decision Tree\n",
        "fpr_dt, tpr_dt, _ = roc_curve(y_test, dt_clf.predict_proba(X_test)[:, 1])\n",
        "auc_dt = auc(fpr_dt, tpr_dt)\n",
        "plt.plot(fpr_dt, tpr_dt, label=f'Decision Tree (AUC = {auc_dt:.2f})')\n",
        "\n",
        "# Plot ROC curve and calculate AUC for Decision Tree\n",
        "fpr_svc, tpr_svc, _ = roc_curve(y_test, best_svc_clf.predict_proba(X_test)[:, 1])\n",
        "auc_svc = auc(fpr_svc, tpr_svc)\n",
        "plt.plot(fpr_svc, tpr_svc, label=f'Tuned SVC with Kernel (AUC = {auc_svc:.2f})')\n",
        "\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "# Show the plot\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B9upEjW9izHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "14n4SSo2verk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all metrics into a DataFrame for easy comparison\n",
        "results_df = pd.DataFrame([dt_metrics, svc_metrics, tuned_svc_metrics])\n",
        "results_df"
      ],
      "metadata": {
        "id": "QTXkWxELve2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Class Experiment\n",
        "Experiment with the above 2 blocks of code; i.e. vanilla SVC and tuned SVC and see for yourself how adding complexity to the dataset impact on the performance of the model. To make the dataset harder to train, you can adjust the class_sep parameter in the make_classification function to decrease the separation between classes. Additionally, you can increase the number of samples and features to make the dataset more complex.\n",
        "\n",
        "make_classification(n_samples=8000, n_features=30,class_sep=0.001, n_classes=2, weights=[0.9, 0.1], random_state=42)"
      ],
      "metadata": {
        "id": "nNTEI8jJfZzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "# URL of the image\n",
        "image_url = \"https://images.shiksha.com/mediadata/ugcDocuments/images/wordpressImages/2022_11_MicrosoftTeams-image-17.jpg\"\n",
        "\n",
        "# Display the image\n",
        "Image(url=image_url)"
      ],
      "metadata": {
        "id": "NHyurERchjG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3B: Build and evaluate other **ensemble models**\n",
        "\n",
        "First we cleanup the memory and restart the session\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yXdFzfsybnmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In Google Colab, we can restart the kernel programmatically using below code. Resetting the kernel is similar to resetting your computer. You will start with a clearn notebook\n",
        "# we restart the kernel to make it simpler to continue and avoid any issue in the code caused by the above blocks of code. It is a good practice!\n",
        "# Restart the kernel\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "0gf_m_tfqpwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Below 1 block of code is a bit of repetition of the first few code blocks in this notebook."
      ],
      "metadata": {
        "id": "QagszhWtvnGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load dataset and balance the data using SMOTE\n",
        "from sklearn.datasets import make_classification\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Generating a synthetic dataset\n",
        "# class_sep   default=1.0 The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task easier.\n",
        "# Learning mode about make_classification here https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html\n",
        "\n",
        "X, y = make_classification(n_samples=5000, n_features=25,class_sep=0.01, n_classes=2, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Balancing the dataset using SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Step 2: Split the dataset into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "#Clean memory up to ensure it won't slow down your work!\n",
        "del X_resampled,y_resampled\n",
        "\n",
        "# Step 3: Build and evaluate the Decision Tree model\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Decision Tree\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "dt_clf.fit(X_train, y_train)\n",
        "dt_preds = dt_clf.predict(X_test)\n",
        "\n",
        "dt_cm = confusion_matrix(y_test, dt_preds)\n",
        "\n",
        "# Evaluation metrics for Decision Tree\n",
        "dt_metrics = {\n",
        "    \"Model\": \"Decision Tree\",\n",
        "    \"Accuracy\": accuracy_score(y_test, dt_preds),\n",
        "    \"Precision\": precision_score(y_test, dt_preds),\n",
        "    \"Recall\": recall_score(y_test, dt_preds),\n",
        "    \"F1 Score\": f1_score(y_test, dt_preds),\n",
        "    \"AUC\": roc_auc_score(y_test, dt_preds)\n",
        "}\n",
        "\n",
        "print(dt_metrics,\"\\n\",\"Confusion Matrix:\\n\", dt_cm)"
      ],
      "metadata": {
        "id": "D6u8O4UMqkYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build and evaluate Random Forest ensemble model\n"
      ],
      "metadata": {
        "id": "LUmcIKVfs8es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and evaluate other ensemble models individually\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "\n",
        "# Random Forest\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "rf_preds = rf_clf.predict(X_test)\n",
        "\n",
        "rf_cm = confusion_matrix(y_test, rf_preds)\n",
        "\n",
        "# Evaluation metrics for Random Forest\n",
        "rf_metrics = {\n",
        "    \"Model\": \"Random Forest\",\n",
        "    \"Accuracy\": accuracy_score(y_test, rf_preds),\n",
        "    \"Precision\": precision_score(y_test, rf_preds),\n",
        "    \"Recall\": recall_score(y_test, rf_preds),\n",
        "    \"F1 Score\": f1_score(y_test, rf_preds),\n",
        "    \"AUC\": roc_auc_score(y_test, rf_preds)\n",
        "}\n",
        "print(rf_metrics,\"\\n\",\"Confusion Matrix:\\n\", rf_cm)\n",
        "\n",
        "print(f\">>>>>>>>>>>>> For {dt_metrics['Model']}, there is a change of F1 score from {round(dt_metrics['F1 Score'],2)} to {round(rf_metrics['F1 Score'],2)} compared to {rf_metrics['Model']} \")\n",
        "\n",
        "# Please note that in this code, we haven't yet tuned the Random Forest. Tuning it, further improve the\n",
        "# performance of the model. But, simply changing decision tree to random forest made a siginificant change in performance"
      ],
      "metadata": {
        "id": "hf-2TUTPbnux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build and evaluate Extra Tree ensemble model\n"
      ],
      "metadata": {
        "id": "oqXAgmzGv1j5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Extra Trees\n",
        "et_clf = ExtraTreesClassifier(random_state=42)\n",
        "et_clf.fit(X_train, y_train)\n",
        "et_preds = et_clf.predict(X_test)\n",
        "\n",
        "et_cm = confusion_matrix(y_test, et_preds)\n",
        "\n",
        "# Evaluation metrics for Extra Trees\n",
        "et_metrics = {\n",
        "    \"Model\": \"Extra Trees\",\n",
        "    \"Accuracy\": accuracy_score(y_test, et_preds),\n",
        "    \"Precision\": precision_score(y_test, et_preds),\n",
        "    \"Recall\": recall_score(y_test, et_preds),\n",
        "    \"F1 Score\": f1_score(y_test, et_preds),\n",
        "    \"AUC\": roc_auc_score(y_test, et_preds)\n",
        "}\n",
        "\n",
        "print(et_metrics,\"\\n\",\"Confusion Matrix:\\n\", et_cm)\n",
        "\n",
        "print(f\">>>>>>>>>>>>> For {dt_metrics['Model']}, there is a change of F1 score from {round(dt_metrics['F1 Score'],2)} to {round(et_metrics['F1 Score'],2)} compared to {et_metrics['Model']} \")\n",
        "\n"
      ],
      "metadata": {
        "id": "buZi5apRauHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build and evaluate Adaboost ensemble model\n",
        "\n"
      ],
      "metadata": {
        "id": "0s4ElKt6wFBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "# AdaBoost\n",
        "adaboost_clf = AdaBoostClassifier(random_state=42)\n",
        "adaboost_clf.fit(X_train, y_train)\n",
        "adaboost_preds = adaboost_clf.predict(X_test)\n",
        "\n",
        "adaboost_cm = confusion_matrix(y_test, adaboost_preds)\n",
        "\n",
        "# Evaluation metrics for AdaBoost\n",
        "adaboost_metrics = {\n",
        "    \"Model\": \"AdaBoost\",\n",
        "    \"Accuracy\": accuracy_score(y_test, adaboost_preds),\n",
        "    \"Precision\": precision_score(y_test, adaboost_preds),\n",
        "    \"Recall\": recall_score(y_test, adaboost_preds),\n",
        "    \"F1 Score\": f1_score(y_test, adaboost_preds),\n",
        "    \"AUC\": roc_auc_score(y_test, adaboost_preds)\n",
        "}\n",
        "\n",
        "print(adaboost_metrics,\"\\n\",\"Confusion Matrix:\\n\", adaboost_cm)\n",
        "\n",
        "print(f\">>>>>>>>>>>>> For {dt_metrics['Model']}, there is a change of F1 score from {round(dt_metrics['F1 Score'],2)} to {round(adaboost_metrics['F1 Score'],2)} compared to {adaboost_metrics['Model']} \")"
      ],
      "metadata": {
        "id": "wQLSnJWJwJFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build and evaluate XGBoost ensemble model\n"
      ],
      "metadata": {
        "id": "9q-5wq5YwCUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# XGBoost\n",
        "xgb_clf = XGBClassifier(random_state=42)\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "xgb_preds = xgb_clf.predict(X_test)\n",
        "\n",
        "xgb_cm = confusion_matrix(y_test, xgb_preds)\n",
        "\n",
        "# Evaluation metrics for XGBoost\n",
        "xgb_metrics = {\n",
        "    \"Model\": \"XGBoost\",\n",
        "    \"Accuracy\": accuracy_score(y_test, xgb_preds),\n",
        "    \"Precision\": precision_score(y_test, xgb_preds),\n",
        "    \"Recall\": recall_score(y_test, xgb_preds),\n",
        "    \"F1 Score\": f1_score(y_test, xgb_preds),\n",
        "    \"AUC\": roc_auc_score(y_test, xgb_preds)\n",
        "}\n",
        "\n",
        "print(xgb_metrics,\"\\n\",\"Confusion Matrix:\\n\", xgb_cm)\n",
        "\n",
        "print(f\">>>>>>>>>>>>> For {dt_metrics['Model']}, there is a change of F1 score from {round(dt_metrics['F1 Score'],2)} to {round(xgb_metrics['F1 Score'],2)} compared to {xgb_metrics['Model']} \")\n"
      ],
      "metadata": {
        "id": "2kUX4ED_weJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare Decision Tree, Random Forest,  Extra Tree, Adaboost, and XGboost"
      ],
      "metadata": {
        "id": "1M5ynK1hvVcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with evaluation metrics for all models\n",
        "results_df = pd.DataFrame([dt_metrics, rf_metrics, et_metrics, adaboost_metrics, xgb_metrics])\n",
        "\n",
        "# Highlight the highest values in each column\n",
        "highlighted_df = results_df.style.highlight_max(axis=0,subset=['Accuracy','Precision','Recall','F1 Score','AUC'])\n",
        "\n",
        "# Display the highlighted DataFrame\n",
        "highlighted_df\n"
      ],
      "metadata": {
        "id": "X3XSq9OwxPPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question for learners:\n",
        "\n",
        "- What do you observe? Summarize your findings\n",
        "- Why do see changes in performance? Discuss it in the class."
      ],
      "metadata": {
        "id": "KMl23I3Ez99w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zfI7jap50GLB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}